# Directed vs Undirected Graphs

## Definitions

- **Directed Graph (Digraph)**: A directed graph is a graph in which the edges have a direction. Each edge connects one vertex to another, and the direction is indicated by an arrow. In a directed graph, if there is an edge from vertex A to vertex B, it doesn't imply that there is an edge from B to A.

- **Undirected Graph**: An undirected graph is a graph in which the edges do not have any direction. The edges simply connect two vertices, and the connection is bidirectional. If there is an edge between vertex A and vertex B, it means you can travel both ways (A to B and B to A).

---

## Comparison Table

| **Feature**               | **Directed Graph**                                                                                                                                      | **Undirected Graph**                                                                            |
| ------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------- |
| **Definition**            | A graph where edges have a direction (from vertex A to B).                                                                                              | A graph where edges do not have a direction.                                                    |
| **Edge Direction**        | Edges have a clear direction (from A to B).                                                                                                             | Edges do not have any direction, they are bidirectional (A to B or B to A).                     |
| **Edge Representation**   | Represented as an ordered pair (A → B).                                                                                                                 | Represented as an unordered pair (A, B).                                                        |
| **Connectivity**          | An edge from A to B does not imply an edge from B to A.                                                                                                 | An edge between A and B implies a two-way connection.                                           |
| **Degree of Vertex**      | Has two types of degrees: in-degree (edges pointing to the vertex) and out-degree (edges pointing away from the vertex).                                | Vertex degree is the total number of edges connected to it.                                     |
| **Cycle**                 | Can have directed cycles where edges form a cycle with direction (e.g., A → B → C → A).                                                                 | Cycles are bidirectional, forming a loop (e.g., A — B — C — A).                                 |
| **Graph Traversal**       | Traversal requires following the direction of edges (DFS, BFS) based on edge direction.                                                                 | Traversal can be done in both directions, making it simpler.                                    |
| **Ease of Understanding** | Slightly more complex due to the direction of edges, requiring attention to the flow.                                                                   | Easier to understand due to bidirectional nature and simpler relationships.                     |
| **Implementation Diff**   | More complex implementation as it requires handling edge directions and possibly additional structures (e.g., adjacency matrix or list with direction). | Simpler to implement since edges are bidirectional, with fewer constraints to handle.           |
| **Use Cases**             | Commonly used for modeling one-way relationships, like web pages, traffic, and data flow.                                                               | Used for modeling bidirectional relationships, like social networks, roads, and communications. |

---

## Example Graphs

### Directed Graph Example

A → B → C ↑ ↓ D ← E

- The edge **A → B** means you can travel from **A** to **B**, but not the other way.
- The edge **D ← E** shows that you can travel from **E** to **D**, but not vice versa.

### Undirected Graph Example

A — B — C | | D — E

- The edge **A — B** implies that there is a two-way connection between **A** and **B**.
- Similarly, **D — E** means you can travel both from **D** to **E** and from **E** to **D**.

# Memory Allocation Techniques: First Fit, Best Fit, Worst Fit

## Problem

Given the following memory map, assume that a new process **P4** comes with memory requirements of **6 KB**. We need to allocate this process to the memory using the following memory allocation strategies:

### Memory Map:

| Block Size | Free Memory Size |
| ---------- | ---------------- |
| 19 KB      |                  |
| 12 KB      |                  |
| 19 KB      |                  |
| 7 KB       |                  |

---

### i) First Fit:

**First Fit** allocates the first free block large enough to accommodate the process.

- Process **P4 (6KB)** will be allocated in the **first free block** that can fit it.

**Memory Allocation Using First Fit:**

| Block Size | Free Memory Size | After Allocation |
| ---------- | ---------------- | ---------------- |
| 19 KB      |                  | 13 KB            |
| 12 KB      |                  | 12 KB            |
| 19 KB      |                  | 19 KB            |
| 7 KB       |                  | 7 KB             |
| **P4** 6KB |                  | **P4** 6KB       |

- **P4** gets allocated to the first free block (19KB), leaving 13KB in that block.

---

### ii) Best Fit:

**Best Fit** allocates the smallest free block that is big enough to accommodate the process.

- **P4 (6KB)** will be allocated in the **smallest free block** that can fit it.

**Memory Allocation Using Best Fit:**

| Block Size | Free Memory Size | After Allocation |
| ---------- | ---------------- | ---------------- |
| 19 KB      |                  | 19 KB            |
| 12 KB      |                  | 6 KB             |
| 19 KB      |                  | 19 KB            |
| 7 KB       |                  | 7 KB             |
| **P4** 6KB |                  | **P4** 6KB       |

- **P4** gets allocated to the **smallest available block** that fits (12KB), leaving 6KB in that block.

---

### iii) Worst Fit:

**Worst Fit** allocates the largest free block to the new process.

- **P4 (6KB)** will be allocated in the **largest free block** available.

**Memory Allocation Using Worst Fit:**

| Block Size | Free Memory Size | After Allocation |
| ---------- | ---------------- | ---------------- |
| 19 KB      |                  | 13 KB            |
| 12 KB      |                  | 12 KB            |
| 19 KB      |                  | 13 KB            |
| 7 KB       |                  | 7 KB             |
| **P4** 6KB |                  | **P4** 6KB       |

- **P4** gets allocated to the **largest block** (19KB), leaving 13KB in that block.

---

## Theory Explanation:

### 1. **First Fit**:

- **Concept**: This algorithm allocates the first free block that is large enough to fit the process.
- **Example**: If we have free blocks of 19KB, 12KB, and 7KB, and the new process requires 6KB, it will be allocated to the **first block** that has enough space (in this case, the 19KB block).
- **Advantages**: Simple to implement, fast allocation because it stops after finding the first fit.
- **Disadvantages**: May leave small, fragmented blocks that can’t be used by larger processes.

### 2. **Best Fit**:

- **Concept**: This algorithm allocates the **smallest free block** that is still large enough to accommodate the process.
- **Example**: If the process requires 6KB, it will be allocated to the **smallest block** that is at least 6KB, even if it leaves larger blocks unused.
- **Advantages**: Tries to minimize wasted memory by utilizing the smallest possible block.
- **Disadvantages**: Can cause increased fragmentation in the long term, as very small blocks may remain after allocation.

### 3. **Worst Fit**:

- **Concept**: This algorithm allocates the **largest free block** available for the new process.
- **Example**: If the process requires 6KB, it will be allocated to the **largest free block**, which in this case is the 19KB block.
- **Advantages**: Leaves larger remaining blocks that might be suitable for other larger processes.
- **Disadvantages**: Can cause inefficient memory usage if the remaining large blocks end up being too fragmented for future use.

---

# Threaded Binary Tree

### Explanation

A **Threaded Binary Tree** is a type of binary tree in which the null pointers of leaf nodes are used to point to the **in-order predecessor** or **in-order successor**. These additional pointers, known as **threads**, help in efficient in-order traversal without using the recursion stack or explicit stack data structures.

## Key Concepts:

1. **Threading**:

   - The null pointers in a traditional binary tree are typically used to denote the absence of a child node.
   - In a **threaded binary tree**, these null pointers are repurposed to store **threads**, which are links that point to the **in-order predecessor** (left thread) or **in-order successor** (right thread) of a node.

2. **In-Order Traversal**:

   - In a threaded binary tree, in-order traversal can be done without recursion or an explicit stack, making it more efficient in terms of memory usage and speed.
   - The threads allow us to directly go to the next node in the in-order traversal sequence.

3. **Types of Threaded Binary Tree**:

   - **Single Threaded Binary Tree**: In this type, either the left or right null pointers are replaced with threads.
     - **Left Threaded**: Left null pointers point to the in-order predecessor.
     - **Right Threaded**: Right null pointers point to the in-order successor.
   - **Double Threaded Binary Tree**: Both left and right null pointers are replaced with threads (both in-order predecessor and successor).

4. **Traversal without Recursion**:
   - The threads allow traversal of the tree without recursion or an explicit stack, which is commonly required in traditional binary trees for in-order traversal.

## Structure of a Threaded Binary Tree Node:

A node in a threaded binary tree contains:

- **Data**: The value stored in the node.
- **Left Pointer**: This pointer can either point to the left child or, if the left child is null, it will point to the in-order predecessor.
- **Right Pointer**: This pointer can either point to the right child or, if the right child is null, it will point to the in-order successor.
- **Left Thread**: A boolean flag to indicate whether the left pointer is a thread (points to the predecessor) or a child pointer.
- **Right Thread**: A boolean flag to indicate whether the right pointer is a thread (points to the successor) or a child pointer.

## Advantages of Threaded Binary Trees:

1. **Efficient In-Order Traversal**: In-order traversal is faster because it doesn’t require recursion or an auxiliary stack, saving space and time.
2. **Memory Efficient**: The tree uses null pointers to store threads, which makes efficient use of the available memory.
3. **Reduced Time Complexity**: Traditional in-order traversal requires visiting nodes through recursion, which can be costly in terms of stack space. In threaded binary trees, traversal can be done iteratively.

## Example:

Consider the following binary tree:

```c
      10
     /  \
    5   15
   / \   / \
  3   7 12  18
```

In a threaded binary tree, instead of using null pointers for the leaf nodes, we would replace them with threads:

- The right null pointer of node `3` would point to `5` (in-order successor).
- The left null pointer of node `7` would point to `5` (in-order predecessor), and so on.

### In-order Traversal Using Threads:

- Start at the leftmost node, and continue to the right by following the threads without needing recursion or a stack.

## Algorithm for In-Order Traversal:

1. Start with the leftmost node (leftmost child).
2. While traversing, follow the right pointers if available. If a thread exists, follow it to the in-order successor.
3. Print the node's data.
4. Repeat until all nodes are visited.

---

### Summary:

A **Threaded Binary Tree** optimizes tree traversal by replacing null pointers with threads to point to the in-order predecessor or successor. This reduces the need for recursion or stacks during traversal and makes the algorithm more efficient in both time and space. It is especially useful for in-order traversal, where it improves performance by allowing iterative traversal.

# Memory Fragmentation

Memory fragmentation refers to the inefficient use of memory in a computer system, which occurs when free memory is divided into small, non-contiguous blocks. This makes it difficult to allocate memory for larger processes, even though the total amount of free memory might be sufficient.

## Types of Memory Fragmentation

1. **External Fragmentation**:

   - Occurs when free memory is divided into small blocks scattered throughout the system.
   - Even though there is enough total free space, it is not contiguous, so it cannot be used for larger memory requests.
   - **Example**: After several processes are loaded and removed from memory, small gaps of unused memory exist between allocated memory regions.

2. **Internal Fragmentation**:
   - Occurs when memory is allocated in fixed-sized blocks, and a process does not use the entire allocated space, leaving unused memory inside each block.
   - This unused space within the allocated block is considered wasted.
   - **Example**: If a process requests 10 KB but the system allocates 16 KB (the smallest available block), the 6 KB of unused space is internally fragmented.

## Impact of Fragmentation

- Fragmentation reduces the efficiency of memory usage and can lead to **memory exhaustion**, where the system runs out of usable memory for new processes, even though there is enough total memory.
- Over time, fragmentation can lead to slower system performance and resource contention.

## Solutions to Memory Fragmentation

- **Compaction**: Moving processes around to consolidate free memory into larger contiguous blocks (more common for external fragmentation).
- **Paging and Segmentation**: Techniques that break memory into fixed-sized or variable-sized chunks, which reduces fragmentation by allocating memory in smaller, manageable units.

# Stack Operations Using Linked List

### Stack Definition:

A stack is a data structure that follows the **LIFO (Last In, First Out)** principle. The two primary operations performed on a stack are:

- **Push**: Add an element to the top of the stack.
- **Pop**: Remove the element from the top of the stack.
- **Display**: Display all elements in the stack.

### Stack Implementation Using Linked List

In this implementation, each node of the linked list represents an element in the stack, and the `top` pointer points to the top node of the stack.

### Algorithm

1. **Push Operation**

   - Add an element to the top of the stack.
   - Create a new node and set its data to the element to be pushed.
   - Set the new node's `next` pointer to the current `top` node.
   - Update the `top` pointer to point to the new node.

2. **Pop Operation**

   - Remove the element from the top of the stack.
   - If the stack is empty, return an error or message indicating that the stack is empty.
   - Set the `top` pointer to the next node in the stack.
   - Free the memory used by the popped node.

3. **Display Operation**
   - Traverse through the stack (linked list) starting from the `top` node.
   - Print the data of each node as you go through the list.

# Merge Sort

### Introduction:

**Merge Sort** is a divide-and-conquer sorting algorithm that divides the input array into two halves, recursively sorts the two halves, and then merges the sorted halves to produce the sorted output. It is an efficient algorithm with a time complexity of O(n log n), which makes it faster than other simple sorting algorithms like Bubble Sort and Insertion Sort for larger datasets.

### Algorithm:

1. **Divide**:
   - If the size of the array is greater than 1, split the array into two halves.
   - Recursively divide each half until each subarray has only one element.
2. **Conquer** (Merge):

   - Merge two sorted halves back together into a single sorted array.
   - Compare the elements of the two halves, adding the smaller element to the result array until all elements from both halves are merged.

3. **Combine**:
   - The merging process is repeated at each level of recursion until the entire array is sorted.

### Merge Sort Algorithm:

```text
MergeSort(arr):
    if length of arr > 1:
        mid = len(arr) // 2  // Find the middle of the array
        left_half = arr[:mid]  // Divide the array into two halves
        right_half = arr[mid:]

        MergeSort(left_half)  // Recursively sort the first half
        MergeSort(right_half) // Recursively sort the second half

        i = j = k = 0

        // Merge the sorted halves
        while i < len(left_half) and j < len(right_half):
            if left_half[i] < right_half[j]:
                arr[k] = left_half[i]
                i += 1
            else:
                arr[k] = right_half[j]
                j += 1
            k += 1

        // Check for any remaining elements in the left_half
        while i < len(left_half):
            arr[k] = left_half[i]
            i += 1
            k += 1

        // Check for any remaining elements in the right_half
        while j < len(right_half):
            arr[k] = right_half[j]
            j += 1
            k += 1
```

# Sorting the Given Data Using Merge Sort

### Given Data:

38, 27, 43, 3, 9, 82, 10

### Merge Sort Algorithm:

1. **Divide** the unsorted list into n sublists, each containing one element.
2. **Repeatedly** merge sublists to produce new sorted sublists until there is only one sublist remaining. This final list is the sorted list.

### Step-by-Step Merge Sort:

#### **Step 1: Split the array into two halves.**

[38, 27, 43] and [3, 9, 82, 10]

#### **Step 2: Recursively split the two halves.**

- For [38, 27, 43]:
  [38] and [27, 43]

  - For [27, 43]:
    [27] and [43]

    - Now merge [27] and [43] to form [27, 43].

  - Merge [38] with [27, 43] to form [27, 38, 43].

- For [3, 9, 82, 10]:
  [3, 9] and [82, 10]

  - For [3, 9]:
    [3] and [9]

    - Now merge [3] and [9] to form [3, 9].

  - For [82, 10]:
    [82] and [10]

    - Now merge [82] and [10] to form [10, 82].

  - Merge [3, 9] with [10, 82] to form [3, 9, 10, 82].

#### **Step 3: Now merge the two sorted halves [27, 38, 43] and [3, 9, 10, 82].**

- Compare the first elements: 27 and 3. Since 3 < 27, add 3 to the result.
- Compare 27 and 9. Since 9 < 27, add 9 to the result.
- Compare 27 and 10. Since 10 < 27, add 10 to the result.
- Compare 27 and 82. Since 27 < 82, add 27 to the result.
- Compare 38 and 82. Since 38 < 82, add 38 to the result.
- Compare 43 and 82. Since 43 < 82, add 43 to the result.
- Add the remaining 82 to the result.

The merged array is:
[3, 9, 10, 27, 38, 43, 82]

### Final Sorted Array:

[3, 9, 10, 27, 38, 43, 82]

# Stack Evaluation of Expression `(2-3+4) * (5+6*7)`

## Problem Statement

Evaluate the following expression using a stack:

## Approach

To evaluate the expression `(2-3+4) * (5+6*7)` using a stack, we need to process the expression step by step, handling operator precedence and parentheses.

### Algorithm

1. **Initialize two stacks**:

   - One for operators (`operators_stack`).
   - One for operands/numbers (`operands_stack`).

2. **Process the expression character by character**:

   - If it's a number, push it onto the `operands_stack`.
   - If it's an operator (`+`, `-`, `*`), push it onto the `operators_stack` after handling operator precedence.
   - If it's a left parenthesis `(`, push it onto the `operators_stack`.
   - If it's a right parenthesis `)`, pop operators from the `operators_stack` and evaluate until a left parenthesis is encountered.

3. **Evaluate operators**: After encountering an operator or closing parenthesis, perform the operation on the top two operands in the `operands_stack` and push the result back onto the stack.

---

## Step-by-Step Evaluation

### Expression: `(2-3+4) * (5+6*7)`

# Stack Evaluation Table

| Step | Expression        | Operators Stack | Operands Stack   | Action                                 |
| ---- | ----------------- | --------------- | ---------------- | -------------------------------------- |
| 1    | (2-3+4) * (5+6*7) |                 |                  | Initialize stacks                      |
| 2    | (2-3+4) * (5+6*7) |                 | [2]              | Push operand 2                         |
| 3    | (2-3+4) * (5+6*7) | [-]             | [2]              | Push operator -                        |
| 4    | (2-3+4) * (5+6*7) | [-]             | [2, 3]           | Push operand 3                         |
| 5    | (2-3+4) * (5+6*7) | [+]             | [-1, 4]          | Evaluate `2 - 3 = -1`, push result -1  |
| 6    | (5+6\*7)          | [+]             | [-1, 4]          | Push operator +                        |
| 7    | (5+6\*7)          | [+]             | [-1, 4, 5]       | Push operand 5                         |
| 8    | (5+6\*7)          | [+]             | [-1, 4, 5, 6]    | Push operand 6                         |
| 9    | (5+6\*7)          | [+ , *]         | [-1, 4, 5, 6]    | Push operator \*                       |
| 10   | (5+6\*7)          | [+ , *]         | [-1, 4, 5, 6, 7] | Push operand 7                         |
| 11   |                   | [+]             | [-1, 4, 47]      | Evaluate `6 * 7 = 42`, push result 42  |
| 12   |                   |                 | [-1, 4, 47]      | Evaluate `5 + 42 = 47`, push result 47 |
| 13   |                   |                 | [141]            | Evaluate `(-1 + 4) * 47 = 141`         |
| 14   |                   |                 | [141]            | Final result is 141                    |

# Postfix Expression Table

| Step | Postfix Expression    | Stack        | Action                                              |
| ---- | --------------------- | ------------ | --------------------------------------------------- |
| 1    | 2 3 - 4 + 5 6 7 _ + _ |              | Initialize stack                                    |
| 2    | 2 3 - 4 + 5 6 7 _ + _ | [2]          | Push operand 2                                      |
| 3    | 3 - 4 + 5 6 7 _ + _   | [2, 3]       | Push operand 3                                      |
| 4    | - 4 + 5 6 7 _ + _     | [2, 3]       | Pop 3 and 2, apply `2 - 3 = -1`, push result -1     |
| 5    | 4 + 5 6 7 _ + _       | [-1]         | Push operand 4                                      |
| 6    | + 5 6 7 _ + _         | [-1, 4]      | Apply `-1 + 4 = 3`, push result 3                   |
| 7    | 5 6 7 _ + _           | [3]          | Push operand 5                                      |
| 8    | 6 7 _ + _             | [3, 5]       | Push operand 6                                      |
| 9    | 7 _ + _               | [3, 5, 6]    | Push operand 7                                      |
| 10   | _ + _                 | [3, 5, 6, 7] | Pop 7 and 6, apply `6 * 7 = 42`, push result 42     |
| 11   | + \*                  | [3, 5, 42]   | Pop 42 and 5, apply `5 + 42 = 47`, push result 47   |
| 12   | \*                    | [3, 47]      | Pop 47 and 3, apply `3 * 47 = 141`, push result 141 |
| 13   |                       | [141]        | Final result is 141                                 |

Postfix Expression: 2 3 - 4 + 5 6 7 \* + \*

# Infix Expression Table

| Step | Infix Expression  | Operand Stack | Operator Stack | Action                                              |
| ---- | ----------------- | ------------- | -------------- | --------------------------------------------------- |
| 1    | (2-3+4) * (5+6*7) |               |                | Initialize stacks                                   |
| 2    | (2-3+4) * (5+6*7) | [2]           |                | Push operand 2                                      |
| 3    | - 3 + 4           | [2]           | [-]            | Push operator `-`                                   |
| 4    | 3 + 4             | [2, 3]        | [-]            | Push operand 3                                      |
| 5    | + 4               | [2, 3]        | [-, +]         | Pop 3 and 2, apply `2 - 3 = -1`, push result -1     |
| 6    | 4                 | [-1]          | [+]            | Push operand 4                                      |
| 7    | + (5 + 6\*7)      | [-1, 4]       | [+]            | Apply `-1 + 4 = 3`, push result 3                   |
| 8    | 5 + (6\*7)        | [3, 5]        | [+]            | Push operand 5                                      |
| 9    | 6 \* 7            | [3, 5, 6]     | [+]            | Push operand 6                                      |
| 10   | \*                | [3, 5, 6]     | [+ , *]        | Apply `6 * 7 = 42`, push result 42                  |
| 11   | + 42              | [3, 5, 42]    | [+ , *]        | Pop 42 and 5, apply `5 + 42 = 47`, push result 47   |
| 12   | \*                | [3, 47]       | [+ , *]        | Pop 47 and 3, apply `3 * 47 = 141`, push result 141 |
| 13   |                   | [141]         |                | Final result is 141                                 |

Prefix Expression: \* + -1 4 + 5 \* 6 7

# Double Ended Queue (Deque)

## Introduction

A **Double Ended Queue** (Deque, pronounced as "deck") is a linear data structure that allows insertion and deletion of elements from both ends — the front and the rear. Unlike a regular queue where you can only enqueue at the rear and dequeue from the front, a deque provides flexibility by allowing operations at both ends.

### Key Characteristics of Deques:

1. **Insertion and Deletion at Both Ends**: Elements can be added or removed from both ends of the deque, which makes it more versatile than a regular queue or stack.
2. **Dynamic Size**: Deques can grow or shrink dynamically, depending on the operations performed.
3. **Access Time**: Access time to both ends of a deque is typically O(1), meaning that insertion, deletion, and access are performed in constant time.

## Operations in Deques

1. **Insert Front (Add First)**:

   - Adds an element to the front of the deque.
   - `O(1)` time complexity.

2. **Insert Rear (Add Last)**:

   - Adds an element to the rear of the deque.
   - `O(1)` time complexity.

3. **Delete Front (Remove First)**:

   - Removes an element from the front of the deque.
   - `O(1)` time complexity.

4. **Delete Rear (Remove Last)**:

   - Removes an element from the rear of the deque.
   - `O(1)` time complexity.

5. **Get Front**:

   - Returns the element at the front of the deque without removing it.
   - `O(1)` time complexity.

6. **Get Rear**:

   - Returns the element at the rear of the deque without removing it.
   - `O(1)` time complexity.

7. **Is Empty**:

   - Checks if the deque is empty.
   - `O(1)` time complexity.

8. **Size**:
   - Returns the number of elements in the deque.
   - `O(1)` time complexity.

## Types of Double Ended Queues

There are a few variations and implementations of deques, depending on how the structure is designed and used. Below are the common types:

### 1. **Input-Restricted Deque**

An **Input-Restricted Deque** is a type of deque where insertions are only allowed at one end (either front or rear), but deletions can still happen from both ends.

- **Insertions**: Restricted to one end (either front or rear).
- **Deletions**: Allowed from both ends.

Example use case:

- **Queue with limited input direction**: You may want to restrict new entries from one side (for example, inserting only from the rear) while still enabling removal of elements from both ends (front and rear).

### 2. **Output-Restricted Deque**

An **Output-Restricted Deque** allows insertions from both ends, but deletions are restricted to one end (either front or rear).

- **Insertions**: Allowed at both ends.
- **Deletions**: Restricted to one end (either front or rear).

Example use case:

- **Stack with limited removal direction**: You may need a deque where elements can be added from both sides but removed only from one side (like a queue that only allows deletion from the front).

### 3. **Circular Deque**

A **Circular Deque** (also known as a circular double-ended queue) is a deque where the last element is connected to the first element, making the deque behave like a circle.

- **Structure**: The front and rear ends are connected circularly.
- **Advantage**: This structure avoids the problem of space wastage that occurs when elements are removed from one end and gaps appear in a linear deque. In a circular deque, the space can be reused.

Operations are similar to a regular deque, but with the added benefit of wrapping around. This implementation is commonly used in applications like a **circular buffer**, where the deque size is fixed, and once the end is reached, it wraps around to the beginning.

Example use case:

- **Circular buffer**: This is used in situations where there is a need to store data in a fixed-size buffer, such as a producer-consumer problem, or in data streaming applications.

### 4. **Array-based Deque**

An **Array-based Deque** is implemented using a dynamic array, where the elements are stored in a contiguous block of memory. The front and rear are represented as indices in the array, and the deque wraps around the array to reuse unused spaces when elements are removed.

- **Advantages**: Constant-time access to both ends of the deque.
- **Challenges**: Resizing the array when the deque becomes full or empty can be costly (O(n)).

### 5. **Linked List-based Deque**

In a **Linked List-based Deque**, each element is stored as a node in a doubly linked list. Each node points to the next node and the previous node, allowing efficient insertion and deletion from both ends.

- **Advantages**: Dynamic size without needing to resize (unlike an array-based deque).
- **Challenges**: Extra memory overhead for storing pointers.

### 6. **Double-ended Priority Queue**

A **Double-ended Priority Queue** (DEPQ) is a variant of the deque where each element is associated with a priority. It allows access to both the smallest and largest element of the queue efficiently.

- **Insertion**: Elements are inserted with a priority value.
- **Deletion**: Elements can be removed from the front (smallest element) or the rear (largest element) based on their priorities.
- **Example use case**: Used in scheduling systems where tasks have both highest and lowest priority levels.

## Applications of Double Ended Queue (Deque)

1. **Palindrome Checking**: A deque can be used to efficiently check whether a string or sequence of characters is a palindrome by removing elements from both ends and comparing them.

2. **Sliding Window Problems**: Deques are useful in problems like finding the maximum or minimum in a sliding window, as they allow fast access to the ends and can store useful elements while discarding irrelevant ones.

3. **Cache Implementations**: A deque can be used to implement a **Least Recently Used (LRU)** cache, where elements are added and removed from both ends of the deque based on usage.

4. **Scheduling and Task Management**: Deques are used in systems where tasks have both high and low priority, and tasks need to be managed from both ends.

5. **Deque as a Queue or Stack**: Depending on how it is used, a deque can be a queue (FIFO) or a stack (LIFO), as it allows operations from both ends.

6. **Game Development**: Deques are used in various game-related scenarios, such as managing characters or events that need to be accessed or modified from both ends (e.g., managing player actions or events in real-time).

# Minimum Spanning Tree for the Graph

## Graph Overview

### Vertices

- V = {1, 2, 3, 4, 5}

### Edges and Weights

- (1, 2) = 1
- (1, 5) = 3
- (1, 3) = 7
- (2, 5) = 2
- (2, 3) = 10
- (5, 4) = 5
- (3, 4) = 2

## Kruskal's Algorithm

Kruskal's Algorithm works by sorting all edges in ascending order of their weights and adding edges to the MST if they don't form a cycle.

### Steps

1. **Sort edges by weight**: [(1,2), (2,5), (3,4), (1,5), (5,4), (1,3), (2,3)]
2. **Initialize MST**: Start with an empty MST.

#### Iteratively Add Edges to MST

- **Edge (1, 2) with weight 1**: Add to MST.
- **Edge (2, 5) with weight 2**: Add to MST.
- **Edge (3, 4) with weight 2**: Add to MST.
- **Edge (1, 5) with weight 3**: Adding this would not create a cycle, add to MST.

Now we have four edges, which is enough to connect all five vertices.

### MST Result (Kruskal's Algorithm)

- Edges in MST: [(1, 2), (2, 5), (3, 4), (1, 5)]
- **Total Cost**: 1 + 2 + 2 + 3 = 8

## Prim's Algorithm

Prim's Algorithm starts from an arbitrary node and grows the MST by adding the minimum weight edge that connects a vertex in the MST to a vertex outside it.

### Steps

1. **Start at vertex 1**.
2. **Choose minimum edge from 1**: Edge (1, 2) with weight 1. Add (1, 2) to MST.
3. **Choose minimum edge connected to 1 or 2**: Edge (2, 5) with weight 2. Add (2, 5) to MST.
4. **Choose minimum edge connected to {1, 2, 5}**: Edge (1, 5) with weight 3. Add (1, 5) to MST.
5. **Choose minimum edge connected to {1, 2, 3, 4, 5}**: Edge (3, 4) with weight 2. Add (3, 4) to MST.

### MST Result (Prim's Algorithm)

- Edges in MST: [(1, 2), (2, 5), (3, 4), (1, 5)]
- **Total Cost**: 1 + 2 + 2 + 3 = 8

## Summary

Both Kruskal's and Prim's algorithms yield the same Minimum Spanning Tree with a total cost of 8.

# AVL Tree Construction

## Definition of AVL Tree

An **AVL tree** is a self-balancing binary search tree (BST) where the difference between the heights of the left and right subtrees of any node (known as the "balance factor") is at most 1. Named after its inventors Adelson-Velsky and Landis, an AVL tree ensures that the height of the tree remains \(O(\log n)\), which keeps search, insertion, and deletion operations efficient.

### AVL Tree Properties

1. **Binary Search Tree (BST)** Property: Left subtree values are less than the node, and right subtree values are greater.
2. **Balance Factor**: The balance factor of each node is calculated as:
   \[
   \text{Balance Factor} = \text{Height of Left Subtree} - \text{Height of Right Subtree}
   \]
   For an AVL tree, the balance factor of each node must be -1, 0, or +1.
3. **Rotations**: When an insertion or deletion operation causes a node to become unbalanced (balance factor not in \([-1, 1]\)), rotations are used to restore balance. There are four types of rotations:
   - **Left Rotation** (LL case)
   - **Right Rotation** (RR case)
   - **Left-Right Rotation** (LR case)
   - **Right-Left Rotation** (RL case)

## Step-by-Step Construction of the AVL Tree

Given data sequence to insert: 30, 20, 10, 25, 40, 50, 55, 22, 23

### Step 1: Insert 30

- Tree:
  ```
      30
  ```

### Step 2: Insert 20

- Tree:
  ```
      30
     /
    20
  ```
- Balance factor of 30 is 1 (no rotation needed).

### Step 3: Insert 10

- Tree:
  ```
      30
     /
    20
   /
  10
  ```
- Balance factor of 30 becomes 2 (unbalanced). **Right Rotation** (RR case) at 30.
- Tree after rotation:
  ```
      20
     /  \
    10   30
  ```

### Step 4: Insert 25

- Tree:
  ```
      20
     /  \
    10   30
         /
        25
  ```
- Balance factors: 20 (0), 30 (1), no rotation needed.

### Step 5: Insert 40

- Tree:
  ```
      20
     /  \
    10   30
         / \
        25  40
  ```
- Balance factors: 20 (0), 30 (0), no rotation needed.

### Step 6: Insert 50

- Tree:
  ```
      20
     /  \
    10   30
         / \
        25  40
              \
               50
  ```
- Balance factor of 30 is -1 and 40 is -1, no rotation needed.

### Step 7: Insert 55

- Tree:
  ```
      20
     /  \
    10   30
         / \
        25  40
              \
               50
                 \
                  55
  ```
- Balance factor of 30 becomes -2 (unbalanced). **Left Rotation** (LL case) at 30.
- Tree after rotation:
  ```
      20
     /  \
    10   40
         /  \
        30   50
       /      \
      25       55
  ```

### Step 8: Insert 22

- Tree:
  ```
      20
     /  \
    10   40
         /  \
        30   50
       /      \
      25       55
     /
    22
  ```
- Balance factors of nodes are within the range \([-1, 1]\), so no rotation is needed.

### Step 9: Insert 23

- Tree:
  ```
      20
     /  \
    10   40
         /  \
        30   50
       /      \
      25       55
     /
    22
     \
      23
  ```
- Balance factor of 30 becomes 2 (unbalanced), and 25 has a balance factor of -1. This requires a **Left-Right Rotation** (LR case) at 30.

#### Left-Right Rotation Steps:

1. **Left Rotation** on 25 and 22:
   ```
      20
     /  \
    10   40
         /  \
        30   50
       /      \
      23       55
     /
    22
   ```
2. **Right Rotation** on 30 and 23:
   ```
      20
     /  \
    10   40
         /  \
        23   50
       /  \    \
      22   30   55
     /
    25
   ```

### Final AVL Tree

The constructed AVL tree is:

```
      20
     /  \
    10   40
         /  \
        23   50
       /  \    \
      22   30   55
     /
    25
```

This AVL tree maintains the height balance property with each node's balance factor in the range \([-1, 1]\).

# Hash Functions and Linear Probing

## Hash Functions

A **hash function** is a function that maps data of arbitrary size to a fixed size, usually to an integer within a specific range. Hash functions are critical in hash tables as they determine where each data element is stored. Here are some common types of hash functions:

# Hashing Methods

## 1. Division Method

The **Division Method** for hash functions is one of the simplest and most widely used methods. The hash value is calculated by dividing the key \( k \) by the number of slots \( m \) in the hash table, and the remainder is used as the hash value.

### Formula:

\[
h(k) = k \mod m
\]
Where:

- \( h(k) \) is the hash value.
- \( k \) is the key.
- \( m \) is the number of slots in the hash table.

### Explanation:

- The idea is to divide the key by the number of slots \( m \), and the remainder of that division gives the position in the table.
- For this method to work effectively and reduce collisions, the number of slots \( m \) should be a **prime number**. This is because if \( m \) is not a prime, some keys may end up being mapped to the same slot (collision), which reduces the effectiveness of the hash function.

### Example:

Let’s say we have a hash table of size \( m = 7 \) (a prime number), and we want to insert the keys: 15, 22, and 31.

- \( h(15) = 15 \mod 7 = 1 \)
- \( h(22) = 22 \mod 7 = 1 \)
- \( h(31) = 31 \mod 7 = 3 \)

So, both 15 and 22 will hash to slot 1, causing a collision. The next step would involve using a collision resolution technique such as linear probing, chaining, etc.

---

## 2. Multiplication Method

The **Multiplication Method** for hash functions uses a multiplication factor to transform the key into a hash value. This method is generally better at distributing keys across the hash table.

### Formula:

\[
h(k) = \text{floor}(m \times (k \times A \mod 1))
\]
Where:

- \( A \) is a constant value, \( 0 < A < 1 \) (typically chosen to be something like \( \frac{\sqrt{5} - 1}{2} \), which is an irrational number, often denoted as the golden ratio).
- \( k \) is the key.
- \( m \) is the number of slots in the hash table.

### Explanation:

- The multiplication method multiplies the key \( k \) by a constant \( A \) and takes the fractional part (after applying the modulus operation). This ensures that keys are distributed more uniformly across the hash table.
- The value is then multiplied by \( m \), and the integer part of this value is returned as the hash value.

This method is typically better for distributing keys evenly across the hash table compared to the division method, especially when the values of \( k \) and \( m \) are not prime.

### Example:

Let’s say \( m = 10 \) (the number of slots in the hash table) and \( A = 0.618033 \) (a constant related to the golden ratio).

For key \( k = 15 \):

- First, calculate \( k \times A = 15 \times 0.618033 = 9.270495 \).
- Take the fractional part: \( 9.270495 - 9 = 0.270495 \).
- Multiply by \( m = 10 \): \( 0.270495 \times 10 = 2.70495 \).
- The integer part of this value is \( 2 \), so \( h(15) = 2 \).

For key \( k = 22 \):

- \( k \times A = 22 \times 0.618033 = 13.596726 \)
- Take the fractional part: \( 13.596726 - 13 = 0.596726 \)
- Multiply by \( m = 10 \): \( 0.596726 \times 10 = 5.96726 \)
- The integer part is \( 5 \), so \( h(22) = 5 \).

---

## 3. Mid-Square Method

The **Mid-Square Method** is another interesting approach where the key is squared, and the middle digits of the squared result are used as the hash value.

### Formula:

- Square the key \( k \), and then extract the middle digits of the result.

### Explanation:

- In the mid-square method, you square the key and then select the middle digits (usually the digits in the middle of the number or a specific number of digits) as the hash value.
- This method works well when the keys have regularity or patterns, and it helps in distributing the keys uniformly in the hash table.

### Example:

Let’s say we have a hash table with \( m = 100 \) slots. For key \( k = 15 \), we square the number and use the middle digits as the hash value.

1. **Square the key**: \( 15^2 = 225 \).
2. Extract the middle digits: In this case, the middle digits of 225 are "25".
3. The hash value is \( h(15) = 25 \).

Let’s try another key, \( k = 23 \).

1. **Square the key**: \( 23^2 = 529 \).
2. Extract the middle digits: The middle digits of 529 are "52".
3. The hash value is \( h(23) = 52 \).

For key \( k = 42 \):

1. **Square the key**: \( 42^2 = 1764 \).
2. Extract the middle digits: The middle digits of 1764 are "76".
3. The hash value is \( h(42) = 76 \).

---

# Summary

| Method                    | Formula                                                 | Explanation                                                                                         | Example                     |
| ------------------------- | ------------------------------------------------------- | --------------------------------------------------------------------------------------------------- | --------------------------- |
| **Division Method**       | \( h(k) = k \mod m \)                                   | Hash value is computed using the remainder of the division of the key by \( m \).                   | \( h(15) = 15 \mod 7 = 1 \) |
| **Multiplication Method** | \( h(k) = \text{floor}(m \times (k \times A \mod 1)) \) | The key is multiplied by a constant \( A \), and the fractional part is used to get the hash value. | \( h(15) = 2 \)             |
| **Mid-Square Method**     | Squared value of key and extracting middle digits.      | Squaring the key and using the middle digits as the hash value.                                     | \( h(15) = 25 \)            |

## Linear Probing

Linear probing is a collision resolution technique in open addressing. When a collision occurs (i.e., when two keys map to the same slot), we move sequentially to the next slot (wrapping around to the beginning of the table if needed) until we find an empty slot.

The formula for linear probing is:

\[

h_i(k) = (h(k) + i) \\mod m

\]

where \( i \) is the number of attempts to find an empty slot.

## Inserting Elements Using Linear Probing

Given a table with 8 slots (\(m = 8\)), we will insert the elements 36, 18, 72, 43, 6, 10, 5, and 15 in the given order.

### Step-by-Step Insertion

### 1. Insert 36

- Hash value: \(36 \\mod 8 = 4\)

- Slot 4 is empty, so insert 36 at index 4.

### 2. Insert 18

- Hash value: \(18 \\mod 8 = 2\)

- Slot 2 is empty, so insert 18 at index 2.

### 3. Insert 72

- Hash value: \(72 \\mod 8 = 0\)

- Slot 0 is empty, so insert 72 at index 0.

### 4. Insert 43

- Hash value: \(43 \\mod 8 = 3\)

- Slot 3 is empty, so insert 43 at index 3.

### 5. Insert 6

- Hash value: \(6 \\mod 8 = 6\)

- Slot 6 is empty, so insert 6 at index 6.

### 6. Insert 10

- Hash value: \(10 \\mod 8 = 2\)

- Slot 2 is occupied (by 18), so we apply linear probing.

  - Try next slot (3): occupied by 43.

  - Try next slot (4): occupied by 36.

  - Try next slot (5): empty.

- Insert 10 at index 5.

### 7. Insert 5

- Hash value: \(5 \\mod 8 = 5\)

- Slot 5 is occupied (by 10), so we apply linear probing.

  - Try next slot (6): occupied by 6.

  - Try next slot (7): empty.

- Insert 5 at index 7.

### 8. Insert 15

- Hash value: \(15 \\mod 8 = 7\)

- Slot 7 is occupied (by 5), so we apply linear probing.

  - Try next slot (0): occupied by 72.

  - Try next slot (1): empty.

- Insert 15 at index 1.

### Final Hash Table

The final state of the hash table is:

| Index | 0 | 1 | 2 | 3 | 4 | 5 | 6 | 7 |

|-------|----|----|----|----|----|----|----|----|

| Value | 72 | 15 | 18 | 43 | 36 | 10 | 6 | 5 |

Each element has been placed according to the hash value and resolved collisions using linear probing.

# Binary Search Tree (BST)

## Definition of Binary Search Tree

A **Binary Search Tree** (BST) is a binary tree with the following properties:

1. The left subtree of a node contains only nodes with values less than the node's key.
2. The right subtree of a node contains only nodes with values greater than the node's key.
3. Both the left and right subtrees must also be binary search trees.

This structure allows efficient searching, insertion, and deletion operations with an average time complexity of \(O(\log n)\).

## Given Traversals

- **In-order**: D B H E A I F J C G
- **Pre-order**: A B D E H C F I J G

To construct a binary search tree from the given **In-order** and **Pre-order** traversals, follow these steps:

## Steps to Construct the BST

1. **Identify the Root**: The first element in the pre-order traversal is always the root of the tree. Here, the root is **A**.

2. **Divide In-order Traversal**: Using the root (A), divide the in-order traversal list into left and right subtrees.

   - In-order: `D B H E` (left subtree), `I F J C G` (right subtree)

3. **Recursively Apply to Subtrees**:
   - Repeat the process for each subtree by taking the next element in the pre-order list as the root for each subtree.

### Constructing the BST

1. **Root Node**:

   - **A** (Root from Pre-order: `A B D E H C F I J G`)
   - Split In-order into left (`D B H E`) and right (`I F J C G`).

2. **Left Subtree of A**:
   - Root: **B** (next in Pre-order)
   - In-order left: `D`
   - In-order right: `H E`
3. **Left Subtree of B**:

   - Root: **D** (next in Pre-order)
   - No left or right children (leaf node).

4. **Right Subtree of B**:
   - Root: **E** (next in Pre-order)
   - In-order left: `H`
   - No right child.
5. **Left Subtree of E**:

   - Root: **H** (next in Pre-order)
   - No left or right children (leaf node).

6. **Right Subtree of A**:

   - Root: **C** (next in Pre-order)
   - In-order left: `I F J`
   - In-order right: `G`

7. **Left Subtree of C**:

   - Root: **F** (next in Pre-order)
   - In-order left: `I`
   - In-order right: `J`

8. **Left Subtree of F**:

   - Root: **I** (next in Pre-order)
   - No left or right children (leaf node).

9. **Right Subtree of F**:

   - Root: **J** (next in Pre-order)
   - No left or right children (leaf node).

10. **Right Subtree of C**:
    - Root: **G** (next in Pre-order)
    - No left or right children (leaf node).

### Final BST Structure

The resulting BST is:

```
         A
       /   \
      B     C
     / \   / \
    D   E F   G
       /   / \
      H   I   J
```

## Post-order Traversal of the Constructed Tree

To determine the **post-order traversal**, visit nodes in the following order: **Left subtree, Right subtree, Root**.

Following this pattern:

1. Traverse the left subtree of **A**:

   - Traverse left subtree of **B**: D
   - Traverse right subtree of **B**:
     - Traverse left subtree of **E**: H
     - Visit **E**
   - Visit **B**

   Result for left subtree of **A**: D H E B

2. Traverse the right subtree of **A**:

   - Traverse left subtree of **C**:
     - Traverse left subtree of **F**: I
     - Traverse right subtree of **F**: J
     - Visit **F**
   - Traverse right subtree of **C**: G
   - Visit **C**

   Result for right subtree of **A**: I J F G C

3. Visit root **A**.

### Final Post-order Traversal

Combining all parts, the post-order traversal is:

```
D H E B I J F G C A
```

# Data Structures and Algorithms Notes

## a) Graph Traversal Algorithms

Graph traversal algorithms are techniques for visiting all nodes in a graph in a specific order. These algorithms are essential in various applications, including pathfinding, connectivity analysis, and searching.

1. **Breadth-First Search (BFS)**:

   - Explores nodes level by level, starting from a source node.

   - Uses a queue to keep track of nodes to visit next.

   - Suitable for finding the shortest path in an unweighted graph.

2. **Depth-First Search (DFS)**:

   - Explores as far as possible down a branch before backtracking.

   - Uses a stack (often implicitly with recursion) to manage node visits.

   - Useful for finding connected components and detecting cycles.

3. **Applications**:

   - Pathfinding in games and maps.

   - Social network analysis.

   - Solving puzzles and maze problems.

4. **Complexity**:

   - Time Complexity: \(O(V + E)\) where \(V\) is the number of vertices and \(E\) is the number of edges.

   - Space Complexity: Depends on the graph representation (e.g., \(O(V)\) for adjacency list).

---

## b) Game Tree

A game tree represents the possible moves in a game, used to make decisions in games like chess, tic-tac-toe, and checkers.

1. **Structure**:

   - Nodes represent game states, and edges represent moves.

   - Root node represents the initial game state; leaf nodes represent possible end states.

2. **Types of Trees**:

   - **Minimax Tree**: Each level alternates between minimizing and maximizing the score.

   - **Alpha-Beta Pruning**: Optimizes Minimax by "pruning" branches that do not affect the final decision.

3. **Purpose**:

   - Helps in finding the optimal move by evaluating possible future moves.

   - Used in artificial intelligence for strategic decision-making.

4. **Complexity**:

   - For chess, the game tree has a large branching factor, making it computationally expensive to evaluate fully.

   - Complexity is reduced using techniques like heuristics and Alpha-Beta pruning.

---

## c) Radix Sort

Radix Sort is a non-comparative sorting algorithm that sorts numbers by processing individual digits. It is particularly efficient for sorting large datasets with uniform-length elements.

1. **Procedure**:

   - Sort numbers starting from the least significant digit (LSD) to the most significant digit (MSD) or vice versa.

   - Uses a stable sorting algorithm (e.g., counting sort) at each digit level.

2. **Types**:

   - **LSD Radix Sort**: Processes digits from least to most significant, suitable for sorting integers.

   - **MSD Radix Sort**: Processes digits from most to least significant, used in some string sorting algorithms.

3. **Applications**:

   - Sorting phone numbers, social security numbers, and other fixed-length data.

   - Efficient for sorting large datasets with integer values.

4. **Complexity**:

   - Time Complexity: \(O(d(n + k))\), where \(d\) is the number of digits, \(n\) is the number of elements, and \(k\) is the base.

   - Space Complexity: \(O(n + k)\).

---

## d) B-tree

A B-tree is a balanced search tree commonly used in databases and file systems. It maintains sorted data and allows for efficient insertion, deletion, and search operations.

1. **Structure**:

   - Each node contains multiple keys and children, allowing for high branching factors.

   - Balanced with all leaf nodes at the same depth.

2. **Properties**:

   - Each node can have a maximum of \(m\) children (where \(m\) is the order of the tree).

   - The root node has at least two children if it’s not a leaf.

3. **Advantages**:

   - Minimizes disk access in databases.

   - Balances itself automatically to maintain performance.

4. **Complexity**:

   - Search, Insert, Delete: \(O(\\log_m n)\) where \(m\) is the branching factor and \(n\) is the number of elements.

---

## e) Round Robin Scheduling

Round Robin Scheduling is a preemptive CPU scheduling algorithm used in operating systems, particularly for time-sharing systems.

1. **Mechanism**:

   - Each process is assigned a fixed time slice or "quantum."

   - Processes are executed in a circular order, and after a process's quantum expires, it goes to the end of the queue.

2. **Advantages**:

   - Fair to all processes as each gets an equal time slice.

   - Simple to implement and works well for time-sharing systems.

3. **Disadvantages**:

   - Performance depends heavily on the length of the quantum.

   - High context-switching overhead if the quantum is too short.

4. **Complexity**:

   - Time Complexity: \(O(n)\) for \(n\) processes per cycle.

   - Throughput and response times are affected by the quantum size.
